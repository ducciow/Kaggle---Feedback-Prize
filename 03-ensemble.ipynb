{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84fafcd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-22T12:09:10.062408Z",
     "iopub.status.busy": "2022-08-22T12:09:10.061704Z",
     "iopub.status.idle": "2022-08-22T12:09:18.696537Z",
     "shell.execute_reply": "2022-08-22T12:09:18.695527Z"
    },
    "papermill": {
     "duration": 8.644592,
     "end_time": "2022-08-22T12:09:18.699089",
     "exception": false,
     "start_time": "2022-08-22T12:09:10.054497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from text_unidecode import unidecode\n",
    "from typing import Tuple\n",
    "import codecs\n",
    "import re\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from functools import partial\n",
    "import datasets\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "INPUT_DIR = \"../input/feedback-prize-effectiveness/\"\n",
    "\n",
    "test_origin = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis]\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28757b8",
   "metadata": {
    "papermill": {
     "duration": 0.004538,
     "end_time": "2022-08-22T12:09:18.708711",
     "exception": false,
     "start_time": "2022-08-22T12:09:18.704173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9536f316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:09:18.719781Z",
     "iopub.status.busy": "2022-08-22T12:09:18.719493Z",
     "iopub.status.idle": "2022-08-22T12:09:18.787966Z",
     "shell.execute_reply": "2022-08-22T12:09:18.787061Z"
    },
    "papermill": {
     "duration": 0.076709,
     "end_time": "2022-08-22T12:09:18.790032",
     "exception": false,
     "start_time": "2022-08-22T12:09:18.713323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"\"\n",
    "    batch_size = 2\n",
    "    max_len = 2048\n",
    "    trn_fold = []\n",
    "    sp_fold = []\n",
    "    num_workers = 1\n",
    "    layer_cls = -4\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start: error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start: error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                               output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [s + (batch_max - len(s)) * [-100] for s in output[\"target\"]]\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "test = test_origin.copy()\n",
    "test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n",
    "test[\"discourse_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"discourse_text\"]]\n",
    "test[\"essay_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"essay_text\"]]\n",
    "\n",
    "discourse_text_values = test['discourse_text'].values\n",
    "essay_text_values = test['essay_text'].values\n",
    "matches = []\n",
    "for i, dt in enumerate(discourse_text_values):\n",
    "    if dt.strip() in essay_text_values[i]:\n",
    "        matches.append(1)\n",
    "    else:\n",
    "        matches.append(0)\n",
    "test['match'] = matches\n",
    "\n",
    "test_grouped_df = test.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "def find_positions(text, discourse_text):\n",
    "\n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "\n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "\n",
    "    for dt in discourse_text:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "\n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1])  # will filter out later\n",
    "            continue\n",
    "            # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "\n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.discourse_type = df['discourse_type'].values\n",
    "        self.discourse_text = df['discourse_text'].values\n",
    "        self.essay_text = df['essay_text'].values\n",
    "        self.essay_ids = df.index.values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.essay_text[index][0]\n",
    "        discourse_text = self.discourse_text[index]\n",
    "\n",
    "        chunks = []\n",
    "        prev = 0\n",
    "\n",
    "        zipped = zip(\n",
    "            find_positions(text, discourse_text),\n",
    "            self.discourse_type[index],\n",
    "        )\n",
    "\n",
    "        for idxs, disc_type in zipped:\n",
    "            # when the discourse_text wasn't found\n",
    "            if idxs == [-1]:\n",
    "                continue\n",
    "            s, e = idxs\n",
    "            # if the start of the current discourse_text is not\n",
    "            # at the end of the previous one.\n",
    "            # (text in between discourse_texts)\n",
    "            if s != prev:\n",
    "                chunks.append(text[prev:s])\n",
    "                prev = s\n",
    "            # if the start of the current discourse_text is\n",
    "            # the same as the end of the previous discourse_text\n",
    "            if s == prev:\n",
    "                chunks.append(cls_tokens_map[disc_type])\n",
    "                chunks.append(text[s:e])\n",
    "                chunks.append(end_tokens_map[disc_type])\n",
    "            prev = e\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            \" \".join(chunks),\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask'],\n",
    "            'essay_id': self.essay_ids[index]\n",
    "        }\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for data in test_loader:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask).to('cpu').numpy()\n",
    "            y_preds = np.pad(y_preds,((0,0),(0,CFG.max_len-y_preds.shape[1]),(0,0)),'constant',constant_values = (0.,0.))\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "    head_preds = []\n",
    "    for i, sample in enumerate(test_dataset):\n",
    "        sample_pred = []\n",
    "        sample_ids = sample['input_ids']\n",
    "        for j, tk_id in enumerate(sample_ids):\n",
    "            if tk_id in cls_ids:\n",
    "                sample_pred.append(predictions[i][j])\n",
    "        head_preds.append(sample_pred)\n",
    "    \n",
    "    final_preds = []\n",
    "    ordered_essay_ids = test['essay_id'].values\n",
    "    disordered_essay_matches = test_grouped_df['match'].values\n",
    "    \n",
    "    pre_essay_id = ''\n",
    "    for essay_id in ordered_essay_ids:\n",
    "        if essay_id == pre_essay_id:\n",
    "            continue\n",
    "        pre_essay_id = essay_id\n",
    "        essay_pred = head_preds[essay_id_map[essay_id]]\n",
    "        essay_macth = disordered_essay_matches[essay_id_map[essay_id]]\n",
    "        for i, discourse_match in enumerate(essay_macth):\n",
    "            if discourse_match == 1:\n",
    "                final_preds.append(essay_pred[i])\n",
    "            else:\n",
    "                final_preds.append([0., 0., 0.])\n",
    "    \n",
    "    return softmax(np.array(final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dddd7d",
   "metadata": {
    "papermill": {
     "duration": 0.004713,
     "end_time": "2022-08-22T12:09:18.800023",
     "exception": false,
     "start_time": "2022-08-22T12:09:18.795310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e43542e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:09:18.812277Z",
     "iopub.status.busy": "2022-08-22T12:09:18.811267Z",
     "iopub.status.idle": "2022-08-22T12:12:53.733786Z",
     "shell.execute_reply": "2022-08-22T12:12:53.732360Z"
    },
    "papermill": {
     "duration": 214.931769,
     "end_time": "2022-08-22T12:12:53.737015",
     "exception": false,
     "start_time": "2022-08-22T12:09:18.805246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/tk-deberta-v2-xlarge-finetuned/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"deberta-v2-xlarge\"\n",
    "CFG.batch_size = 2\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": False})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        cls_embeddings = out.last_hidden_state\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=CFG.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "tk_deberta_preds_v2 = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    tk_deberta_preds_v2.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "model_preds_1 = np.mean(tk_deberta_preds_v2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9f33c",
   "metadata": {
    "papermill": {
     "duration": 0.005063,
     "end_time": "2022-08-22T12:12:53.750378",
     "exception": false,
     "start_time": "2022-08-22T12:12:53.745315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5529f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:12:53.762645Z",
     "iopub.status.busy": "2022-08-22T12:12:53.762347Z",
     "iopub.status.idle": "2022-08-22T12:14:39.818460Z",
     "shell.execute_reply": "2022-08-22T12:14:39.817277Z"
    },
    "papermill": {
     "duration": 106.06541,
     "end_time": "2022-08-22T12:14:39.821079",
     "exception": false,
     "start_time": "2022-08-22T12:12:53.755669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/tk-deberta-v3-large-finetuned/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"deberta-v3-large\"\n",
    "CFG.batch_size = 2\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": False})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        cls_embeddings = out.last_hidden_state\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=CFG.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "tk_deberta_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    tk_deberta_preds.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "model_preds_2 = np.mean(tk_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7d192",
   "metadata": {
    "papermill": {
     "duration": 0.005088,
     "end_time": "2022-08-22T12:14:39.831990",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.826902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Longformer large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59adbc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:14:39.844693Z",
     "iopub.status.busy": "2022-08-22T12:14:39.844210Z",
     "iopub.status.idle": "2022-08-22T12:14:39.851911Z",
     "shell.execute_reply": "2022-08-22T12:14:39.851112Z"
    },
    "papermill": {
     "duration": 0.016101,
     "end_time": "2022-08-22T12:14:39.854048",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.837947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/tk-longformer-large-finetuned/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"longformer-large\"\n",
    "# CFG.batch_size = 2\n",
    "# CFG.trn_fold = [0, 1, 3, 4]\n",
    "# CFG.sp_fold = [4]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "    \n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# cls_id_map = {\n",
    "#     label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "# }\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                           batch_size=CFG.batch_size,\n",
    "#                           shuffle=False,\n",
    "#                           collate_fn=collate_fn,\n",
    "#                           num_workers=CFG.num_workers,\n",
    "#                           pin_memory=True,\n",
    "#                           drop_last=False)\n",
    "\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "# essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "\n",
    "# longformer_preds = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "    \n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     if fold in CFG.sp_fold:\n",
    "#         state = torch.load(f\"../input/new-tk-longformer-large/{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                            map_location=torch.device('cpu'))\n",
    "#     else:\n",
    "#         state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                            map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     longformer_preds.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_3 = np.mean(longformer_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07371723",
   "metadata": {
    "papermill": {
     "duration": 0.004938,
     "end_time": "2022-08-22T12:14:39.864022",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.859084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09645d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:14:39.875746Z",
     "iopub.status.busy": "2022-08-22T12:14:39.875466Z",
     "iopub.status.idle": "2022-08-22T12:14:39.903602Z",
     "shell.execute_reply": "2022-08-22T12:14:39.902730Z"
    },
    "papermill": {
     "duration": 0.036555,
     "end_time": "2022-08-22T12:14:39.905627",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.869072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"\"\n",
    "    batch_size = 16\n",
    "    max_len = 512\n",
    "    trn_fold = []\n",
    "    num_workers = 1\n",
    "    layer_cls = -4\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "test = test_origin.copy()\n",
    "test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df['text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.text[item],\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "\n",
    "        return samples\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "def inference_fn(test_loader, model):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for data in test_loader:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask)\n",
    "        y_preds = softmax(y_preds.to('cpu').numpy())\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969c5a8",
   "metadata": {
    "papermill": {
     "duration": 0.005002,
     "end_time": "2022-08-22T12:14:39.915882",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.910880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3df4844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:14:39.928046Z",
     "iopub.status.busy": "2022-08-22T12:14:39.927767Z",
     "iopub.status.idle": "2022-08-22T12:14:39.933041Z",
     "shell.execute_reply": "2022-08-22T12:14:39.932078Z"
    },
    "papermill": {
     "duration": 0.014016,
     "end_time": "2022-08-22T12:14:39.935054",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.921038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/sq-deberta-v2-xlarge/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"microsoft/deberta-v2-xlarge\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": True})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         all_hidden_states = torch.stack(out.hidden_states)\n",
    "#         cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# sq_deberta_preds_v2 = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     sq_deberta_preds_v2.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_4 = np.mean(sq_deberta_preds_v2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955b58",
   "metadata": {
    "papermill": {
     "duration": 0.005015,
     "end_time": "2022-08-22T12:14:39.945313",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.940298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9b6368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:14:39.957045Z",
     "iopub.status.busy": "2022-08-22T12:14:39.956724Z",
     "iopub.status.idle": "2022-08-22T12:16:25.237421Z",
     "shell.execute_reply": "2022-08-22T12:16:25.236205Z"
    },
    "papermill": {
     "duration": 105.289634,
     "end_time": "2022-08-22T12:16:25.240006",
     "exception": false,
     "start_time": "2022-08-22T12:14:39.950372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/sq-deberta-v3-large-new/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"microsoft/deberta-v3-large\"\n",
    "CFG.batch_size = 16\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "CFG.sp_fold = [0]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": True})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        all_hidden_states = torch.stack(out.hidden_states)\n",
    "        cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "SEP = tokenizer.sep_token\n",
    "test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "sq_deberta_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    if fold in CFG.sp_fold:\n",
    "        swa_model = AveragedModel(model)\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best_swa.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        swa_model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, swa_model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del swa_model, state, prediction\n",
    "    else:\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_preds_5 = np.mean(sq_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce68b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:16:25.253490Z",
     "iopub.status.busy": "2022-08-22T12:16:25.253173Z",
     "iopub.status.idle": "2022-08-22T12:16:25.259055Z",
     "shell.execute_reply": "2022-08-22T12:16:25.258118Z"
    },
    "papermill": {
     "duration": 0.014691,
     "end_time": "2022-08-22T12:16:25.261081",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.246390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/feedback-deberta-v3-large-sep/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"microsoft/deberta-v3-large\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": True})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         all_hidden_states = torch.stack(out.hidden_states)\n",
    "#         cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# sq_deberta_preds = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     swa_model = AveragedModel(model)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     swa_model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, swa_model)\n",
    "#     sq_deberta_preds.append(prediction)\n",
    "#     del model, swa_model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_5 = np.mean(sq_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec21d9",
   "metadata": {
    "papermill": {
     "duration": 0.005242,
     "end_time": "2022-08-22T12:16:25.271690",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.266448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159f7866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:16:25.283991Z",
     "iopub.status.busy": "2022-08-22T12:16:25.283712Z",
     "iopub.status.idle": "2022-08-22T12:16:25.288613Z",
     "shell.execute_reply": "2022-08-22T12:16:25.287654Z"
    },
    "papermill": {
     "duration": 0.013431,
     "end_time": "2022-08-22T12:16:25.290668",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.277237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/sq-roberta-large/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"roberta-large\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state[:, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# roberta_predictions = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     roberta_predictions.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_6 = np.mean(roberta_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503df0e",
   "metadata": {
    "papermill": {
     "duration": 0.005296,
     "end_time": "2022-08-22T12:16:25.301397",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.296101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19312e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:16:25.313668Z",
     "iopub.status.busy": "2022-08-22T12:16:25.313403Z",
     "iopub.status.idle": "2022-08-22T12:16:25.318411Z",
     "shell.execute_reply": "2022-08-22T12:16:25.317555Z"
    },
    "papermill": {
     "duration": 0.013504,
     "end_time": "2022-08-22T12:16:25.320520",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.307016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# from sklearn.metrics import log_loss\n",
    "# import gensim\n",
    "# from scipy import sparse\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54a0f84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:16:25.332921Z",
     "iopub.status.busy": "2022-08-22T12:16:25.332649Z",
     "iopub.status.idle": "2022-08-22T12:16:25.340655Z",
     "shell.execute_reply": "2022-08-22T12:16:25.339789Z"
    },
    "papermill": {
     "duration": 0.016928,
     "end_time": "2022-08-22T12:16:25.342954",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.326026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPUT_DIR = \"../input/feedback-prize-effectiveness/\"\n",
    "\n",
    "# class CFG:\n",
    "#     seed = 42\n",
    "#     n_folds = 4\n",
    "\n",
    "# def get_train_essay(essay_id):\n",
    "#     essay_path = os.path.join(INPUT_DIR,f'train/{essay_id}.txt')\n",
    "#     essay_text = open(essay_path,'r').read()\n",
    "#     return essay_text\n",
    "\n",
    "# def get_test_essay(essay_id):\n",
    "#     essay_path = os.path.join(INPUT_DIR,f'test/{essay_id}.txt')\n",
    "#     essay_text = open(essay_path,'r').read()\n",
    "#     return essay_text\n",
    "\n",
    "# train = pd.read_csv(INPUT_DIR+'train.csv')\n",
    "# test = pd.read_csv(INPUT_DIR+'test.csv')\n",
    "# train['essay_text'] = train['essay_id'].apply(get_train_essay)\n",
    "# test['essay_text'] = test['essay_id'].apply(get_test_essay)\n",
    "\n",
    "# effectiveness_map = {'Ineffective':0, 'Adequate':1, 'Effective':2}\n",
    "# train['target'] = train['discourse_effectiveness'].map(effectiveness_map)\n",
    "\n",
    "# sgkf = StratifiedGroupKFold(n_splits=CFG.n_folds,shuffle=True,random_state=CFG.seed)\n",
    "# for fold, (_,val_idx) in enumerate(sgkf.split(X=train, y=train['target'], groups=train.essay_id)):\n",
    "#     train.loc[val_idx,'kfold'] = fold\n",
    "\n",
    "# word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../input/google-news/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# def avg_feature_vector(sentence, model, num_features):\n",
    "#     words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n",
    "#     feature_vec = np.zeros((num_features,),dtype=\"float32\")\n",
    "#     i=0\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             feature_vec = np.add(feature_vec, model[word])\n",
    "#         except KeyError as error:\n",
    "#             feature_vec \n",
    "#             i = i + 1\n",
    "#     if len(words) > 0:\n",
    "#         feature_vec = np.divide(feature_vec, len(words)- i)\n",
    "#     return feature_vec\n",
    "\n",
    "# params = {}\n",
    "# params[\"objective\"] = 'multiclass'\n",
    "# params['metric'] = 'multi_logloss'\n",
    "# params['boosting'] = 'gbdt'\n",
    "# params['num_class'] = 3\n",
    "# params['is_unbalance'] = True\n",
    "# params[\"learning_rate\"] = 0.05\n",
    "# params[\"lambda_l2\"] = 0.0256\n",
    "# params[\"num_leaves\"] = 52\n",
    "# params[\"max_depth\"] = 10\n",
    "# params[\"feature_fraction\"] = 0.503\n",
    "# params[\"bagging_fraction\"] = 0.741\n",
    "# params[\"bagging_freq\"] = 8\n",
    "# params[\"bagging_seed\"] = 10\n",
    "# params[\"min_data_in_leaf\"] = 10\n",
    "# params[\"verbosity\"] = -1\n",
    "# params[\"random_state\"] = 42\n",
    "# num_rounds = 1000\n",
    "\n",
    "# oof_score = 0\n",
    "# y_test_pred = np.zeros((test.shape[0], 3))\n",
    "\n",
    "\n",
    "# for fold in range(CFG.n_folds):\n",
    "#     print(f'=============fold:{fold}==================')\n",
    "#     train_fold=train[train['kfold']!=fold].reset_index(drop=True)\n",
    "#     valid_fold=train[train['kfold']==fold].reset_index(drop=True)\n",
    "\n",
    "#     #word2vec\n",
    "\n",
    "#     #discourse_text\n",
    "#     word2vec_train_disc_text = np.zeros((len(train_fold.index),300),dtype=\"float32\")\n",
    "#     word2vec_valid_disc_text = np.zeros((len(valid_fold.index),300),dtype=\"float32\")\n",
    "#     word2vec_test_disc_text = np.zeros((len(test.index),300),dtype=\"float32\")\n",
    "#     for i in range(len(train_fold.index)):\n",
    "#         word2vec_train_disc_text[i] = avg_feature_vector(train_fold[\"discourse_text\"][i], word2vec_model, 300)\n",
    "#     for i in range(len(valid_fold.index)):\n",
    "#         word2vec_valid_disc_text[i] = avg_feature_vector(valid_fold[\"discourse_text\"][i], word2vec_model, 300)\n",
    "#     for i in range(len(test.index)):\n",
    "#         word2vec_test_disc_text[i] = avg_feature_vector(test[\"discourse_text\"][i], word2vec_model, 300)\n",
    "\n",
    "#     #essay_text\n",
    "#     word2vec_train_essay_text = np.zeros((len(train_fold.index),300),dtype=\"float32\")\n",
    "#     word2vec_valid_essay_text = np.zeros((len(valid_fold.index),300),dtype=\"float32\")\n",
    "#     word2vec_test_essay_text = np.zeros((len(test.index),300),dtype=\"float32\")\n",
    "#     for i in range(len(train_fold.index)):\n",
    "#         word2vec_train_essay_text[i] = avg_feature_vector(train_fold[\"essay_text\"][i], word2vec_model, 300)\n",
    "#     for i in range(len(valid_fold.index)):\n",
    "#         word2vec_valid_essay_text[i] = avg_feature_vector(valid_fold[\"essay_text\"][i], word2vec_model, 300)\n",
    "#     for i in range(len(test.index)):\n",
    "#         word2vec_test_essay_text[i] = avg_feature_vector(test[\"essay_text\"][i], word2vec_model, 300)\n",
    "\n",
    "#     #OneHot\n",
    "#     ohe = OneHotEncoder()\n",
    "#     train_type_ohe=sparse.csr_matrix(ohe.fit_transform(train_fold['discourse_type'].values.reshape(-1,1)))\n",
    "#     valid_type_ohe=sparse.csr_matrix(ohe.transform(valid_fold['discourse_type'].values.reshape(-1,1)))\n",
    "#     test_type_ohe=sparse.csr_matrix(ohe.transform(test['discourse_type'].values.reshape(-1,1)))\n",
    "\n",
    "\n",
    "#     #merge\n",
    "#     Xtrain_word2vec = sparse.hstack((train_type_ohe,word2vec_train_disc_text,word2vec_train_essay_text))\n",
    "#     Xvalid_word2vec = sparse.hstack((valid_type_ohe,word2vec_valid_disc_text,word2vec_valid_essay_text))\n",
    "#     test_word2vec = sparse.hstack((test_type_ohe,word2vec_test_disc_text,word2vec_test_essay_text))\n",
    "\n",
    "#     #lgbm\n",
    "#     lgtrain = lgb.Dataset(Xtrain_word2vec, label=train_fold['target'].ravel())\n",
    "#     lgvalidation = lgb.Dataset(Xvalid_word2vec, label=valid_fold['target'].ravel())\n",
    "\n",
    "#     model = lgb.train(params, lgtrain, num_rounds, \n",
    "#                     valid_sets=[lgtrain, lgvalidation], \n",
    "#                     early_stopping_rounds=100, verbose_eval=100)\n",
    "\n",
    "#     y_pred = model.predict(Xvalid_word2vec, num_iteration=model.best_iteration)\n",
    "#     y_test_pred += model.predict(test_word2vec, num_iteration=model.best_iteration)\n",
    "\n",
    "#     score = log_loss(valid_fold['target'], y_pred)\n",
    "#     oof_score += score\n",
    "\n",
    "#     print(f'Fold:{fold},valid score:{score}')\n",
    "    \n",
    "# y_test_pred = y_test_pred / float(CFG.n_folds)\n",
    "# oof_score /= float(CFG.n_folds)\n",
    "# print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2722ee",
   "metadata": {
    "papermill": {
     "duration": 0.005598,
     "end_time": "2022-08-22T12:16:25.354063",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.348465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127ae832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T12:16:25.366996Z",
     "iopub.status.busy": "2022-08-22T12:16:25.366687Z",
     "iopub.status.idle": "2022-08-22T12:16:25.401068Z",
     "shell.execute_reply": "2022-08-22T12:16:25.400134Z"
    },
    "papermill": {
     "duration": 0.043481,
     "end_time": "2022-08-22T12:16:25.403237",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.359756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>0.417678</td>\n",
       "      <td>0.565180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>0.030528</td>\n",
       "      <td>0.779688</td>\n",
       "      <td>0.189785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>0.531241</td>\n",
       "      <td>0.434868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.108738</td>\n",
       "      <td>0.584731</td>\n",
       "      <td>0.306531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>0.651635</td>\n",
       "      <td>0.254960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2e214524dbe3</td>\n",
       "      <td>0.024255</td>\n",
       "      <td>0.492763</td>\n",
       "      <td>0.482982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84812fc2ab9f</td>\n",
       "      <td>0.024173</td>\n",
       "      <td>0.441420</td>\n",
       "      <td>0.534407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c668ff840720</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.666032</td>\n",
       "      <td>0.274060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739a6d00f44a</td>\n",
       "      <td>0.064306</td>\n",
       "      <td>0.568164</td>\n",
       "      <td>0.367530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcfae2c9a244</td>\n",
       "      <td>0.021968</td>\n",
       "      <td>0.638130</td>\n",
       "      <td>0.339902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276     0.017142  0.417678   0.565180\n",
       "1  5a88900e7dc1     0.030528  0.779688   0.189785\n",
       "2  9790d835736b     0.033891  0.531241   0.434868\n",
       "3  75ce6d68b67b     0.108738  0.584731   0.306531\n",
       "4  93578d946723     0.093405  0.651635   0.254960\n",
       "5  2e214524dbe3     0.024255  0.492763   0.482982\n",
       "6  84812fc2ab9f     0.024173  0.441420   0.534407\n",
       "7  c668ff840720     0.059908  0.666032   0.274060\n",
       "8  739a6d00f44a     0.064306  0.568164   0.367530\n",
       "9  bcfae2c9a244     0.021968  0.638130   0.339902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m1 = 0.33  # v2 tk\n",
    "m2 = 0.34  # v3 tk\n",
    "# m3 = 0.06  # longformer tk\n",
    "# m4 = 0.22  # v2 sq\n",
    "m5 = 0.33  # v3 sq\n",
    "# m6 = 0.06  # roberta sq\n",
    "# m7 = 0.07  # lgbm\n",
    "\n",
    "preds_Ineffective = model_preds_1[:, 2] * m1 + model_preds_2[:, 2] * m2 + model_preds_5[:, 0] * m5 #+ y_test_pred[:, 0] * m7\n",
    "preds_Adequate = model_preds_1[:, 0] * m1 + model_preds_2[:, 0] * m2 + model_preds_5[:, 1] * m5 #+ y_test_pred[:, 1] * m7\n",
    "preds_Effective = model_preds_1[:, 1] * m1 + model_preds_2[:, 1] * m2 + model_preds_5[:, 2] * m5 #+ y_test_pred[:, 2] * m7\n",
    "\n",
    "sample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n",
    "\n",
    "sample['Ineffective'] = preds_Ineffective\n",
    "sample['Adequate'] = preds_Adequate\n",
    "sample['Effective'] = preds_Effective\n",
    "\n",
    "sample.to_csv('submission.csv', index=False)\n",
    "\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34177b21",
   "metadata": {
    "papermill": {
     "duration": 0.005758,
     "end_time": "2022-08-22T12:16:25.414694",
     "exception": false,
     "start_time": "2022-08-22T12:16:25.408936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 446.443962,
   "end_time": "2022-08-22T12:16:28.820918",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-22T12:09:02.376956",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
