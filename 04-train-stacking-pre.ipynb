{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836c1b54",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:06.096576Z",
     "iopub.status.busy": "2022-08-22T09:34:06.096146Z",
     "iopub.status.idle": "2022-08-22T09:34:14.517679Z",
     "shell.execute_reply": "2022-08-22T09:34:14.516652Z"
    },
    "papermill": {
     "duration": 8.431494,
     "end_time": "2022-08-22T09:34:14.520163",
     "exception": false,
     "start_time": "2022-08-22T09:34:06.088669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from text_unidecode import unidecode\n",
    "from typing import Tuple\n",
    "import codecs\n",
    "import re\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from functools import partial\n",
    "import datasets\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "INPUT_DIR = \"../input/feedback-prize-effectiveness/\"\n",
    "\n",
    "FOLD = 4\n",
    "\n",
    "train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for fold, (train_id, val_id) in enumerate(gkf.split(X=train, y=train.discourse_effectiveness, groups=train.essay_id)):\n",
    "    train.loc[val_id, \"fold\"] = int(fold)\n",
    "train[\"fold\"] = train[\"fold\"].astype(int)\n",
    "\n",
    "test_origin = train[train.fold == FOLD].reset_index(drop=True)\n",
    "\n",
    "# test_origin = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis]\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8af31",
   "metadata": {
    "papermill": {
     "duration": 0.004712,
     "end_time": "2022-08-22T09:34:14.530227",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.525515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b1655d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:14.541847Z",
     "iopub.status.busy": "2022-08-22T09:34:14.541557Z",
     "iopub.status.idle": "2022-08-22T09:34:14.551753Z",
     "shell.execute_reply": "2022-08-22T09:34:14.550873Z"
    },
    "papermill": {
     "duration": 0.018756,
     "end_time": "2022-08-22T09:34:14.553694",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.534938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     model = \"\"\n",
    "#     batch_size = 2\n",
    "#     max_len = 2048\n",
    "#     trn_fold = []\n",
    "#     sp_fold = []\n",
    "#     num_workers = 1\n",
    "#     layer_cls = -4\n",
    "\n",
    "# def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "#     return error.object[error.start: error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "# def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "#     return error.object[error.start: error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# # Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "# codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "# codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "# def resolve_encodings_and_normalize(text: str) -> str:\n",
    "#     \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "#     text = (\n",
    "#         text.encode(\"raw_unicode_escape\")\n",
    "#             .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "#             .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "#             .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "#     )\n",
    "#     text = unidecode(text)\n",
    "#     return text\n",
    "\n",
    "# def get_essay(essay_id, is_train=True):\n",
    "#     parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "#     essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "#     essay_text = open(essay_path, 'r').read()\n",
    "#     return essay_text\n",
    "\n",
    "# class Collate:\n",
    "#     def __init__(self, tokenizer, isTrain=True):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.isTrain = isTrain\n",
    "\n",
    "#     def __call__(self, batch):\n",
    "#         output = dict()\n",
    "#         output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "#         output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "#         if self.isTrain:\n",
    "#             output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "#         # calculate max token length of this batch\n",
    "#         batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "#         # add padding\n",
    "#         output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "#                                output[\"input_ids\"]]\n",
    "#         output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "\n",
    "#         # convert to tensors\n",
    "#         output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "#         output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "#         if self.isTrain:\n",
    "#             output[\"target\"] = [s + (batch_max - len(s)) * [-100] for s in output[\"target\"]]\n",
    "#             output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "#         return output\n",
    "\n",
    "# test = test_origin.copy()\n",
    "# test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=True))\n",
    "# test[\"discourse_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"discourse_text\"]]\n",
    "# test[\"essay_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"essay_text\"]]\n",
    "\n",
    "# discourse_text_values = test['discourse_text'].values\n",
    "# essay_text_values = test['essay_text'].values\n",
    "# matches = []\n",
    "# for i, dt in enumerate(discourse_text_values):\n",
    "#     if dt.strip() in essay_text_values[i]:\n",
    "#         matches.append(1)\n",
    "#     else:\n",
    "#         matches.append(0)\n",
    "# test['match'] = matches\n",
    "\n",
    "# test_grouped_df = test.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "# disc_types = [\n",
    "#     \"Claim\",\n",
    "#     \"Concluding Statement\",\n",
    "#     \"Counterclaim\",\n",
    "#     \"Evidence\",\n",
    "#     \"Lead\",\n",
    "#     \"Position\",\n",
    "#     \"Rebuttal\",\n",
    "# ]\n",
    "# cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "# end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "# label2id = {\n",
    "#     \"Adequate\": 0,\n",
    "#     \"Effective\": 1,\n",
    "#     \"Ineffective\": 2,\n",
    "# }\n",
    "\n",
    "# def find_positions(text, discourse_text):\n",
    "\n",
    "#     # keeps track of what has already\n",
    "#     # been located\n",
    "#     min_idx = 0\n",
    "\n",
    "#     # stores start and end indexes of discourse_texts\n",
    "#     idxs = []\n",
    "\n",
    "#     for dt in discourse_text:\n",
    "#         # calling strip is essential\n",
    "#         matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "\n",
    "#         # If there are multiple matches, take the first one\n",
    "#         # that is past the previous discourse texts.\n",
    "#         if len(matches) > 1:\n",
    "#             for m in matches:\n",
    "#                 if m.start() >= min_idx:\n",
    "#                     break\n",
    "#         # If no matches are found\n",
    "#         elif len(matches) == 0:\n",
    "#             idxs.append([-1])  # will filter out later\n",
    "#             continue\n",
    "#             # If one match is found\n",
    "#         else:\n",
    "#             m = matches[0]\n",
    "\n",
    "#         idxs.append([m.start(), m.end()])\n",
    "\n",
    "#         min_idx = m.start()\n",
    "\n",
    "#     return idxs\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer):\n",
    "#         self.df = df\n",
    "#         self.discourse_type = df['discourse_type'].values\n",
    "#         self.discourse_text = df['discourse_text'].values\n",
    "#         self.essay_text = df['essay_text'].values\n",
    "#         self.essay_ids = df.index.values\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         text = self.essay_text[index][0]\n",
    "#         discourse_text = self.discourse_text[index]\n",
    "\n",
    "#         chunks = []\n",
    "#         prev = 0\n",
    "\n",
    "#         zipped = zip(\n",
    "#             find_positions(text, discourse_text),\n",
    "#             self.discourse_type[index],\n",
    "#         )\n",
    "\n",
    "#         for idxs, disc_type in zipped:\n",
    "#             # when the discourse_text wasn't found\n",
    "#             if idxs == [-1]:\n",
    "#                 continue\n",
    "#             s, e = idxs\n",
    "#             # if the start of the current discourse_text is not\n",
    "#             # at the end of the previous one.\n",
    "#             # (text in between discourse_texts)\n",
    "#             if s != prev:\n",
    "#                 chunks.append(text[prev:s])\n",
    "#                 prev = s\n",
    "#             # if the start of the current discourse_text is\n",
    "#             # the same as the end of the previous discourse_text\n",
    "#             if s == prev:\n",
    "#                 chunks.append(cls_tokens_map[disc_type])\n",
    "#                 chunks.append(text[s:e])\n",
    "#                 chunks.append(end_tokens_map[disc_type])\n",
    "#             prev = e\n",
    "\n",
    "#         tokenized = self.tokenizer(\n",
    "#             \" \".join(chunks),\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=CFG.max_len,\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': tokenized['input_ids'],\n",
    "#             'attention_mask': tokenized['attention_mask'],\n",
    "#             'essay_id': self.essay_ids[index]\n",
    "#         }\n",
    "\n",
    "\n",
    "# def inference_fn(test_loader, model):\n",
    "#     preds = []\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     for data in test_loader:\n",
    "#         ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#         mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#         with torch.no_grad():\n",
    "#             y_preds = model(ids, mask).to('cpu').numpy()\n",
    "#             y_preds = np.pad(y_preds,((0,0),(0,CFG.max_len-y_preds.shape[1]),(0,0)),'constant',constant_values = (0.,0.))\n",
    "#         preds.append(y_preds)\n",
    "#     predictions = np.concatenate(preds)\n",
    "    \n",
    "#     head_preds = []\n",
    "#     for i, sample in enumerate(test_dataset):\n",
    "#         sample_pred = []\n",
    "#         sample_ids = sample['input_ids']\n",
    "#         for j, tk_id in enumerate(sample_ids):\n",
    "#             if tk_id in cls_ids:\n",
    "#                 sample_pred.append(predictions[i][j])\n",
    "#         head_preds.append(sample_pred)\n",
    "    \n",
    "#     final_preds = []\n",
    "#     ordered_essay_ids = test['essay_id'].values\n",
    "#     disordered_essay_matches = test_grouped_df['match'].values\n",
    "    \n",
    "#     pre_essay_id = ''\n",
    "#     for essay_id in ordered_essay_ids:\n",
    "#         if essay_id == pre_essay_id:\n",
    "#             continue\n",
    "#         pre_essay_id = essay_id\n",
    "#         essay_pred = head_preds[essay_id_map[essay_id]]\n",
    "#         essay_macth = disordered_essay_matches[essay_id_map[essay_id]]\n",
    "#         for i, discourse_match in enumerate(essay_macth):\n",
    "#             if discourse_match == 1:\n",
    "#                 final_preds.append(essay_pred[i])\n",
    "#             else:\n",
    "#                 final_preds.append([0., 0., 0.])\n",
    "    \n",
    "#     return softmax(np.array(final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de19f1c",
   "metadata": {
    "papermill": {
     "duration": 0.00452,
     "end_time": "2022-08-22T09:34:14.563039",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.558519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9b9e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:14.574596Z",
     "iopub.status.busy": "2022-08-22T09:34:14.573912Z",
     "iopub.status.idle": "2022-08-22T09:34:14.579905Z",
     "shell.execute_reply": "2022-08-22T09:34:14.579101Z"
    },
    "papermill": {
     "duration": 0.013809,
     "end_time": "2022-08-22T09:34:14.581846",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.568037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/tk-deberta-v2-xlarge-finetuned/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"deberta-v2-xlarge\"\n",
    "# CFG.batch_size = 2\n",
    "# CFG.trn_fold = [FOLD]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "    \n",
    "    \n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# cls_id_map = {\n",
    "#     label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "# }\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                           batch_size=CFG.batch_size,\n",
    "#                           shuffle=False,\n",
    "#                           collate_fn=collate_fn,\n",
    "#                           num_workers=CFG.num_workers,\n",
    "#                           pin_memory=True,\n",
    "#                           drop_last=False)\n",
    "\n",
    "\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "# essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "# tk_deberta_preds_v2 = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     tk_deberta_preds_v2.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_1 = np.mean(tk_deberta_preds_v2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68594ff",
   "metadata": {
    "papermill": {
     "duration": 0.004618,
     "end_time": "2022-08-22T09:34:14.591233",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.586615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52480cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:14.602206Z",
     "iopub.status.busy": "2022-08-22T09:34:14.601925Z",
     "iopub.status.idle": "2022-08-22T09:34:14.606868Z",
     "shell.execute_reply": "2022-08-22T09:34:14.605906Z"
    },
    "papermill": {
     "duration": 0.013159,
     "end_time": "2022-08-22T09:34:14.609168",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.596009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/tk-deberta-v3-large-finetuned/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"deberta-v3-large\"\n",
    "# CFG.batch_size = 2\n",
    "# CFG.trn_fold = [FOLD]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "    \n",
    "    \n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# cls_id_map = {\n",
    "#     label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "# }\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                           batch_size=CFG.batch_size,\n",
    "#                           shuffle=False,\n",
    "#                           collate_fn=collate_fn,\n",
    "#                           num_workers=CFG.num_workers,\n",
    "#                           pin_memory=True,\n",
    "#                           drop_last=False)\n",
    "\n",
    "\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "# essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "# tk_deberta_preds = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     tk_deberta_preds.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_2 = np.mean(tk_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5fbf1",
   "metadata": {
    "papermill": {
     "duration": 0.004603,
     "end_time": "2022-08-22T09:34:14.618827",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.614224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Longformer large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147f8e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:14.629744Z",
     "iopub.status.busy": "2022-08-22T09:34:14.629491Z",
     "iopub.status.idle": "2022-08-22T09:34:14.634719Z",
     "shell.execute_reply": "2022-08-22T09:34:14.633792Z"
    },
    "papermill": {
     "duration": 0.013025,
     "end_time": "2022-08-22T09:34:14.636630",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.623605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/tk-longformer-large-finetuned/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"longformer-large\"\n",
    "# CFG.batch_size = 2\n",
    "# CFG.trn_fold = [FOLD]\n",
    "# CFG.sp_fold = [4]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "    \n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# cls_id_map = {\n",
    "#     label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "# }\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                           batch_size=CFG.batch_size,\n",
    "#                           shuffle=False,\n",
    "#                           collate_fn=collate_fn,\n",
    "#                           num_workers=CFG.num_workers,\n",
    "#                           pin_memory=True,\n",
    "#                           drop_last=False)\n",
    "\n",
    "# cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "# essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "\n",
    "# longformer_preds = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "    \n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     if fold in CFG.sp_fold:\n",
    "#         state = torch.load(f\"../input/new-tk-longformer-large/{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                            map_location=torch.device('cpu'))\n",
    "#     else:\n",
    "#         state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                            map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     longformer_preds.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_3 = np.mean(longformer_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d8059",
   "metadata": {
    "papermill": {
     "duration": 0.004693,
     "end_time": "2022-08-22T09:34:14.646543",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.641850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65176c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:14.659060Z",
     "iopub.status.busy": "2022-08-22T09:34:14.658786Z",
     "iopub.status.idle": "2022-08-22T09:34:20.069000Z",
     "shell.execute_reply": "2022-08-22T09:34:20.067596Z"
    },
    "papermill": {
     "duration": 5.420224,
     "end_time": "2022-08-22T09:34:20.072141",
     "exception": false,
     "start_time": "2022-08-22T09:34:14.651917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"\"\n",
    "    batch_size = 16\n",
    "    max_len = 512\n",
    "    trn_fold = []\n",
    "    num_workers = 1\n",
    "    layer_cls = -4\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "test = test_origin.copy()\n",
    "test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=True))\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df['text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.text[item],\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "\n",
    "        return samples\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "def inference_fn(test_loader, model):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for data in test_loader:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask)\n",
    "        y_preds = softmax(y_preds.to('cpu').numpy())\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36e4fe",
   "metadata": {
    "papermill": {
     "duration": 0.007515,
     "end_time": "2022-08-22T09:34:20.087638",
     "exception": false,
     "start_time": "2022-08-22T09:34:20.080123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a836b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:20.104826Z",
     "iopub.status.busy": "2022-08-22T09:34:20.104528Z",
     "iopub.status.idle": "2022-08-22T09:34:20.111511Z",
     "shell.execute_reply": "2022-08-22T09:34:20.110524Z"
    },
    "papermill": {
     "duration": 0.018626,
     "end_time": "2022-08-22T09:34:20.114314",
     "exception": false,
     "start_time": "2022-08-22T09:34:20.095688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/sq-deberta-v2-xlarge/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"microsoft/deberta-v2-xlarge\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [FOLD]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": True})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         all_hidden_states = torch.stack(out.hidden_states)\n",
    "#         cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# sq_deberta_preds_v2 = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     sq_deberta_preds_v2.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_4 = np.mean(sq_deberta_preds_v2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7069bfb",
   "metadata": {
    "papermill": {
     "duration": 0.008541,
     "end_time": "2022-08-22T09:34:20.131504",
     "exception": false,
     "start_time": "2022-08-22T09:34:20.122963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84bb815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:34:20.150631Z",
     "iopub.status.busy": "2022-08-22T09:34:20.150077Z",
     "iopub.status.idle": "2022-08-22T09:44:18.561452Z",
     "shell.execute_reply": "2022-08-22T09:44:18.560278Z"
    },
    "papermill": {
     "duration": 598.424118,
     "end_time": "2022-08-22T09:44:18.564049",
     "exception": false,
     "start_time": "2022-08-22T09:34:20.139931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/sq-deberta-v3-large-new/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"microsoft/deberta-v3-large\"\n",
    "CFG.batch_size = 16\n",
    "CFG.trn_fold = [FOLD]\n",
    "CFG.sp_fold = [0]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": True})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        all_hidden_states = torch.stack(out.hidden_states)\n",
    "        cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "SEP = tokenizer.sep_token\n",
    "test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "sq_deberta_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    if fold in CFG.sp_fold:\n",
    "        swa_model = AveragedModel(model)\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best_swa.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        swa_model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, swa_model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del swa_model, state, prediction\n",
    "    else:\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_preds_5 = np.mean(sq_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b123d2d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:44:18.576571Z",
     "iopub.status.busy": "2022-08-22T09:44:18.576270Z",
     "iopub.status.idle": "2022-08-22T09:44:18.581611Z",
     "shell.execute_reply": "2022-08-22T09:44:18.580598Z"
    },
    "papermill": {
     "duration": 0.014077,
     "end_time": "2022-08-22T09:44:18.583960",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.569883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/feedback-deberta-v3-large-sep/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"microsoft/deberta-v3-large\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [FOLD]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": True})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         all_hidden_states = torch.stack(out.hidden_states)\n",
    "#         cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# sq_deberta_preds = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     swa_model = AveragedModel(model)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     swa_model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, swa_model)\n",
    "#     sq_deberta_preds.append(prediction)\n",
    "#     del model, swa_model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_5 = np.mean(sq_deberta_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823e22b",
   "metadata": {
    "papermill": {
     "duration": 0.004643,
     "end_time": "2022-08-22T09:44:18.593804",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.589161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### roberta large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b757d6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:44:18.604792Z",
     "iopub.status.busy": "2022-08-22T09:44:18.604491Z",
     "iopub.status.idle": "2022-08-22T09:44:18.610552Z",
     "shell.execute_reply": "2022-08-22T09:44:18.609716Z"
    },
    "papermill": {
     "duration": 0.013795,
     "end_time": "2022-08-22T09:44:18.612495",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.598700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"../input/sq-roberta-large/\"\n",
    "# CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "# CFG.model = \"roberta-large\"\n",
    "# CFG.batch_size = 16\n",
    "# CFG.trn_fold = [FOLD]\n",
    "\n",
    "\n",
    "# class FeedBackModel(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(FeedBackModel, self).__init__()\n",
    "#         self.config = torch.load(config_path)\n",
    "#         self.config.update({\"output_hidden_states\": False})\n",
    "#         self.model = AutoModel.from_config(self.config)\n",
    "#         self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "#     def forward(self, ids, mask):\n",
    "#         out = self.model(input_ids=ids, attention_mask=mask)\n",
    "#         cls_embeddings = out.last_hidden_state[:, 0]\n",
    "#         outputs = self.fc(cls_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "# SEP = tokenizer.sep_token\n",
    "# test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "# collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "# test_dataset = TestDataset(test, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset,\n",
    "#                          batch_size=CFG.batch_size,\n",
    "#                          shuffle=False,\n",
    "#                          collate_fn=collate_fn,\n",
    "#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "# roberta_predictions = []\n",
    "# for fold in CFG.trn_fold:\n",
    "#     print(\"Fold {}\".format(fold))\n",
    "\n",
    "#     model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "#     state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "#                        map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     prediction = inference_fn(test_loader, model)\n",
    "#     roberta_predictions.append(prediction)\n",
    "#     del model, state, prediction\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_6 = np.mean(roberta_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd4c94",
   "metadata": {
    "papermill": {
     "duration": 0.004869,
     "end_time": "2022-08-22T09:44:18.622298",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.617429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe53d365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:44:18.633591Z",
     "iopub.status.busy": "2022-08-22T09:44:18.633107Z",
     "iopub.status.idle": "2022-08-22T09:44:18.640873Z",
     "shell.execute_reply": "2022-08-22T09:44:18.639860Z"
    },
    "papermill": {
     "duration": 0.015994,
     "end_time": "2022-08-22T09:44:18.643430",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.627436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mpdel_preds = [model_preds_1, model_preds_2, model_preds_3, model_preds_4, model_preds_5]\n",
    "# for i, preds in enumerate(mpdel_preds):\n",
    "#     col_1 = 'preds' + str(i + 1) + '_' + str(1)\n",
    "#     col_2 = 'preds' + str(i + 1) + '_' + str(2)\n",
    "#     col_3 = 'preds' + str(i + 1) + '_' + str(3)\n",
    "#     test_origin[col_1] = preds[:, 0]\n",
    "#     test_origin[col_2] = preds[:, 1]\n",
    "#     test_origin[col_3] = preds[:, 2]\n",
    "\n",
    "col_1 = 'preds' + str(5) + '_' + str(1)\n",
    "col_2 = 'preds' + str(5) + '_' + str(2)\n",
    "col_3 = 'preds' + str(5) + '_' + str(3)\n",
    "test_origin[col_1] = model_preds_5[:, 0]\n",
    "test_origin[col_2] = model_preds_5[:, 1]\n",
    "test_origin[col_3] = model_preds_5[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd57fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:44:18.655412Z",
     "iopub.status.busy": "2022-08-22T09:44:18.654759Z",
     "iopub.status.idle": "2022-08-22T09:44:18.732475Z",
     "shell.execute_reply": "2022-08-22T09:44:18.731649Z"
    },
    "papermill": {
     "duration": 0.086037,
     "end_time": "2022-08-22T09:44:18.734546",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.648509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_origin.to_csv(f'train_further_deb_{FOLD}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16006f",
   "metadata": {
    "papermill": {
     "duration": 0.005148,
     "end_time": "2022-08-22T09:44:18.744652",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.739504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8a7f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T09:44:18.755696Z",
     "iopub.status.busy": "2022-08-22T09:44:18.755438Z",
     "iopub.status.idle": "2022-08-22T09:44:18.759873Z",
     "shell.execute_reply": "2022-08-22T09:44:18.758925Z"
    },
    "papermill": {
     "duration": 0.012092,
     "end_time": "2022-08-22T09:44:18.761807",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.749715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m1 = 0.22  # v2 tk\n",
    "# m2 = 0.22  # v3 tk\n",
    "# m3 = 0.06  # longformer tk\n",
    "# m4 = 0.22  # v2 sq\n",
    "# m5 = 0.22  # v3 sq\n",
    "\n",
    "# preds_Ineffective = model_preds_1[:, 2] * m1 + model_preds_2[:, 2] * m2 + model_preds_4[:, 0] * m4 + model_preds_5[:, 0] * m5 + model_preds_3[:, 2] * m3\n",
    "# preds_Adequate = model_preds_1[:, 0] * m1 + model_preds_2[:, 0] * m2 + model_preds_4[:, 1] * m4 + model_preds_5[:, 1] * m5 + model_preds_3[:, 0] * m3\n",
    "# preds_Effective = model_preds_1[:, 1] * m1 + model_preds_2[:, 1] * m2 + model_preds_4[:, 2] * m4 + model_preds_5[:, 2] * m5 + model_preds_3[:, 1] * m3\n",
    "\n",
    "# sample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n",
    "\n",
    "# sample['Ineffective'] = preds_Ineffective\n",
    "# sample['Adequate'] = preds_Adequate\n",
    "# sample['Effective'] = preds_Effective\n",
    "\n",
    "# sample.to_csv('submission.csv', index=False)\n",
    "\n",
    "# display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ed3a3",
   "metadata": {
    "papermill": {
     "duration": 0.004696,
     "end_time": "2022-08-22T09:44:18.771657",
     "exception": false,
     "start_time": "2022-08-22T09:44:18.766961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 623.593758,
   "end_time": "2022-08-22T09:44:22.203894",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-22T09:33:58.610136",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
