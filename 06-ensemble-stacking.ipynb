{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f2a62e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-23T08:36:14.186502Z",
     "iopub.status.busy": "2022-08-23T08:36:14.185501Z",
     "iopub.status.idle": "2022-08-23T08:36:16.597208Z",
     "shell.execute_reply": "2022-08-23T08:36:16.596206Z"
    },
    "papermill": {
     "duration": 2.422076,
     "end_time": "2022-08-23T08:36:16.600135",
     "exception": false,
     "start_time": "2022-08-23T08:36:14.178059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from text_unidecode import unidecode\n",
    "from typing import Tuple\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "INPUT_DIR = \"../input/feedback-prize-effectiveness/\"\n",
    "\n",
    "test_origin = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis]  # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis]\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e2431",
   "metadata": {
    "papermill": {
     "duration": 0.004694,
     "end_time": "2022-08-23T08:36:16.610302",
     "exception": false,
     "start_time": "2022-08-23T08:36:16.605608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d37dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:36:16.622029Z",
     "iopub.status.busy": "2022-08-23T08:36:16.621249Z",
     "iopub.status.idle": "2022-08-23T08:36:16.689321Z",
     "shell.execute_reply": "2022-08-23T08:36:16.688449Z"
    },
    "papermill": {
     "duration": 0.076407,
     "end_time": "2022-08-23T08:36:16.691439",
     "exception": false,
     "start_time": "2022-08-23T08:36:16.615032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"\"\n",
    "    batch_size = 2\n",
    "    max_len = 2048\n",
    "    trn_fold = []\n",
    "    sp_fold = []\n",
    "    num_workers = 1\n",
    "    layer_cls = -4\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start: error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start: error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                               output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [s + (batch_max - len(s)) * [-100] for s in output[\"target\"]]\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "test = test_origin.copy()\n",
    "test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n",
    "test[\"discourse_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"discourse_text\"]]\n",
    "test[\"essay_text\"] = [resolve_encodings_and_normalize(x) for x in test[\"essay_text\"]]\n",
    "\n",
    "discourse_text_values = test['discourse_text'].values\n",
    "essay_text_values = test['essay_text'].values\n",
    "matches = []\n",
    "for i, dt in enumerate(discourse_text_values):\n",
    "    if dt.strip() in essay_text_values[i]:\n",
    "        matches.append(1)\n",
    "    else:\n",
    "        matches.append(0)\n",
    "test['match'] = matches\n",
    "\n",
    "test_grouped_df = test.groupby([\"essay_id\"]).agg(list)\n",
    "\n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "\n",
    "def find_positions(text, discourse_text):\n",
    "\n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "\n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "\n",
    "    for dt in discourse_text:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "\n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([-1])  # will filter out later\n",
    "            continue\n",
    "            # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "\n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.discourse_type = df['discourse_type'].values\n",
    "        self.discourse_text = df['discourse_text'].values\n",
    "        self.essay_text = df['essay_text'].values\n",
    "        self.essay_ids = df.index.values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.essay_text[index][0]\n",
    "        discourse_text = self.discourse_text[index]\n",
    "\n",
    "        chunks = []\n",
    "        prev = 0\n",
    "\n",
    "        zipped = zip(\n",
    "            find_positions(text, discourse_text),\n",
    "            self.discourse_type[index],\n",
    "        )\n",
    "\n",
    "        for idxs, disc_type in zipped:\n",
    "            # when the discourse_text wasn't found\n",
    "            if idxs == [-1]:\n",
    "                continue\n",
    "            s, e = idxs\n",
    "            # if the start of the current discourse_text is not\n",
    "            # at the end of the previous one.\n",
    "            # (text in between discourse_texts)\n",
    "            if s != prev:\n",
    "                chunks.append(text[prev:s])\n",
    "                prev = s\n",
    "            # if the start of the current discourse_text is\n",
    "            # the same as the end of the previous discourse_text\n",
    "            if s == prev:\n",
    "                chunks.append(cls_tokens_map[disc_type])\n",
    "                chunks.append(text[s:e])\n",
    "                chunks.append(end_tokens_map[disc_type])\n",
    "            prev = e\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            \" \".join(chunks),\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask'],\n",
    "            'essay_id': self.essay_ids[index]\n",
    "        }\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for data in test_loader:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask).to('cpu').numpy()\n",
    "            y_preds = np.pad(y_preds,((0,0),(0,CFG.max_len-y_preds.shape[1]),(0,0)),'constant',constant_values = (0.,0.))\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "    head_preds = []\n",
    "    for i, sample in enumerate(test_dataset):\n",
    "        sample_pred = []\n",
    "        sample_ids = sample['input_ids']\n",
    "        for j, tk_id in enumerate(sample_ids):\n",
    "            if tk_id in cls_ids:\n",
    "                sample_pred.append(predictions[i][j])\n",
    "        head_preds.append(sample_pred)\n",
    "    \n",
    "    final_preds = []\n",
    "    ordered_essay_ids = test['essay_id'].values\n",
    "    disordered_essay_matches = test_grouped_df['match'].values\n",
    "    \n",
    "    pre_essay_id = ''\n",
    "    for essay_id in ordered_essay_ids:\n",
    "        if essay_id == pre_essay_id:\n",
    "            continue\n",
    "        pre_essay_id = essay_id\n",
    "        essay_pred = head_preds[essay_id_map[essay_id]]\n",
    "        essay_macth = disordered_essay_matches[essay_id_map[essay_id]]\n",
    "        for i, discourse_match in enumerate(essay_macth):\n",
    "            if discourse_match == 1:\n",
    "                final_preds.append(essay_pred[i])\n",
    "            else:\n",
    "                final_preds.append([0., 0., 0.])\n",
    "    \n",
    "    return softmax(np.array(final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6be6ac",
   "metadata": {
    "papermill": {
     "duration": 0.004559,
     "end_time": "2022-08-23T08:36:16.700865",
     "exception": false,
     "start_time": "2022-08-23T08:36:16.696306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12bbfa04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:36:16.712055Z",
     "iopub.status.busy": "2022-08-23T08:36:16.711468Z",
     "iopub.status.idle": "2022-08-23T08:39:44.635618Z",
     "shell.execute_reply": "2022-08-23T08:39:44.634505Z"
    },
    "papermill": {
     "duration": 207.932757,
     "end_time": "2022-08-23T08:39:44.638359",
     "exception": false,
     "start_time": "2022-08-23T08:36:16.705602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/tk-deberta-v2-xlarge-finetuned/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"deberta-v2-xlarge\"\n",
    "CFG.batch_size = 2\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": False})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        cls_embeddings = out.last_hidden_state\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=CFG.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "tk_deberta_preds_v2 = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    tk_deberta_preds_v2.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_1 = np.mean(tk_deberta_preds_v2, axis=0)\n",
    "model_preds_1 = np.array(tk_deberta_preds_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f0f1d",
   "metadata": {
    "papermill": {
     "duration": 0.005262,
     "end_time": "2022-08-23T08:39:44.649540",
     "exception": false,
     "start_time": "2022-08-23T08:39:44.644278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab657aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:39:44.661143Z",
     "iopub.status.busy": "2022-08-23T08:39:44.660818Z",
     "iopub.status.idle": "2022-08-23T08:41:37.828037Z",
     "shell.execute_reply": "2022-08-23T08:41:37.826935Z"
    },
    "papermill": {
     "duration": 113.175933,
     "end_time": "2022-08-23T08:41:37.830458",
     "exception": false,
     "start_time": "2022-08-23T08:39:44.654525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/tk-deberta-v3-large-finetuned/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"deberta-v3-large\"\n",
    "CFG.batch_size = 2\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": False})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        cls_embeddings = out.last_hidden_state\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=CFG.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "    \n",
    "tk_deberta_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    tk_deberta_preds.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_2 = np.mean(tk_deberta_preds, axis=0)\n",
    "model_preds_2 = np.array(tk_deberta_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673353c9",
   "metadata": {
    "papermill": {
     "duration": 0.006096,
     "end_time": "2022-08-23T08:41:37.842338",
     "exception": false,
     "start_time": "2022-08-23T08:41:37.836242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Longformer large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320a80dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:41:37.854244Z",
     "iopub.status.busy": "2022-08-23T08:41:37.853922Z",
     "iopub.status.idle": "2022-08-23T08:43:30.296889Z",
     "shell.execute_reply": "2022-08-23T08:43:30.295797Z"
    },
    "papermill": {
     "duration": 112.452197,
     "end_time": "2022-08-23T08:43:30.299765",
     "exception": false,
     "start_time": "2022-08-23T08:41:37.847568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/tk-longformer-large-finetuned/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"longformer-large\"\n",
    "CFG.batch_size = 2\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "CFG.sp_fold = [4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": False})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        cls_embeddings = out.last_hidden_state\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1] for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test_grouped_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=CFG.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "cls_ids = set(list(cls_id_map.values()))\n",
    "\n",
    "essay_id_map = {v['essay_id'] : k for k, v in enumerate(test_dataset)}\n",
    "\n",
    "longformer_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "    \n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    if fold in CFG.sp_fold:\n",
    "        state = torch.load(f\"../input/new-tk-longformer-large/{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    longformer_preds.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "# model_preds_3 = np.mean(longformer_preds, axis=0)\n",
    "model_preds_3 = np.array(longformer_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0a1f2",
   "metadata": {
    "papermill": {
     "duration": 0.005421,
     "end_time": "2022-08-23T08:43:30.311001",
     "exception": false,
     "start_time": "2022-08-23T08:43:30.305580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcf2fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:43:30.323082Z",
     "iopub.status.busy": "2022-08-23T08:43:30.322760Z",
     "iopub.status.idle": "2022-08-23T08:43:30.350356Z",
     "shell.execute_reply": "2022-08-23T08:43:30.349400Z"
    },
    "papermill": {
     "duration": 0.036364,
     "end_time": "2022-08-23T08:43:30.352585",
     "exception": false,
     "start_time": "2022-08-23T08:43:30.316221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"\"\n",
    "    batch_size = 16\n",
    "    max_len = 512\n",
    "    trn_fold = []\n",
    "    num_workers = 1\n",
    "    layer_cls = -4\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "test = test_origin.copy()\n",
    "test['essay_text'] = test['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df['text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.text[item],\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=CFG.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "\n",
    "        return samples\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "def inference_fn(test_loader, model):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for data in test_loader:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(ids, mask)\n",
    "        y_preds = softmax(y_preds.to('cpu').numpy())\n",
    "        preds.append(y_preds)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13cac5",
   "metadata": {
    "papermill": {
     "duration": 0.005698,
     "end_time": "2022-08-23T08:43:30.364167",
     "exception": false,
     "start_time": "2022-08-23T08:43:30.358469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v2 xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e4c7cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:43:30.376034Z",
     "iopub.status.busy": "2022-08-23T08:43:30.375739Z",
     "iopub.status.idle": "2022-08-23T08:47:18.387821Z",
     "shell.execute_reply": "2022-08-23T08:47:18.386654Z"
    },
    "papermill": {
     "duration": 228.020791,
     "end_time": "2022-08-23T08:47:18.390266",
     "exception": false,
     "start_time": "2022-08-23T08:43:30.369475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/sq-deberta-v2-xlarge/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"microsoft/deberta-v2-xlarge\"\n",
    "CFG.batch_size = 16\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": True})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        all_hidden_states = torch.stack(out.hidden_states)\n",
    "        cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "SEP = tokenizer.sep_token\n",
    "test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "sq_deberta_preds_v2 = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model)\n",
    "    sq_deberta_preds_v2.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_4 = np.mean(sq_deberta_preds_v2, axis=0)\n",
    "model_preds_4 = np.array(sq_deberta_preds_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba254a5c",
   "metadata": {
    "papermill": {
     "duration": 0.005459,
     "end_time": "2022-08-23T08:47:18.401710",
     "exception": false,
     "start_time": "2022-08-23T08:47:18.396251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deberta v3 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c0f7b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:47:18.415435Z",
     "iopub.status.busy": "2022-08-23T08:47:18.413936Z",
     "iopub.status.idle": "2022-08-23T08:49:05.952582Z",
     "shell.execute_reply": "2022-08-23T08:49:05.951474Z"
    },
    "papermill": {
     "duration": 107.547962,
     "end_time": "2022-08-23T08:49:05.955130",
     "exception": false,
     "start_time": "2022-08-23T08:47:18.407168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"../input/sq-deberta-v3-large-new/\"\n",
    "CONFIG_PATH = MODEL_PATH + 'config.pth'\n",
    "\n",
    "CFG.model = \"microsoft/deberta-v3-large\"\n",
    "CFG.batch_size = 16\n",
    "CFG.trn_fold = [0, 1, 3, 4]\n",
    "CFG.sp_fold = [0]\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = torch.load(config_path)\n",
    "        self.config.update({\"output_hidden_states\": True})\n",
    "        self.model = AutoModel.from_config(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask)\n",
    "        all_hidden_states = torch.stack(out.hidden_states)\n",
    "        cls_embeddings = all_hidden_states[CFG.layer_cls, :, 0]\n",
    "        outputs = self.fc(cls_embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + 'tokenizer', use_fast=True)\n",
    "\n",
    "SEP = tokenizer.sep_token\n",
    "test['text'] = test['discourse_type'] + ' ' + test['discourse_text'] + SEP + test['essay_text']\n",
    "\n",
    "collate_fn = Collate(tokenizer, isTrain=False)\n",
    "\n",
    "test_dataset = TestDataset(test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn,\n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "\n",
    "sq_deberta_preds = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(\"Fold {}\".format(fold))\n",
    "\n",
    "    model = FeedBackModel(config_path=CONFIG_PATH)\n",
    "    if fold in CFG.sp_fold:\n",
    "        swa_model = AveragedModel(model)\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best_swa.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        swa_model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, swa_model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del swa_model, state, prediction\n",
    "    else:\n",
    "        state = torch.load(MODEL_PATH + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                           map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model)\n",
    "        sq_deberta_preds.append(prediction)\n",
    "        del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# model_preds_5 = np.mean(sq_deberta_preds, axis=0)\n",
    "model_preds_5 = np.array(sq_deberta_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad69ad",
   "metadata": {
    "papermill": {
     "duration": 0.00591,
     "end_time": "2022-08-23T08:49:05.968047",
     "exception": false,
     "start_time": "2022-08-23T08:49:05.962137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d2f988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:05.981288Z",
     "iopub.status.busy": "2022-08-23T08:49:05.980913Z",
     "iopub.status.idle": "2022-08-23T08:49:10.895696Z",
     "shell.execute_reply": "2022-08-23T08:49:10.894663Z"
    },
    "papermill": {
     "duration": 4.924226,
     "end_time": "2022-08-23T08:49:10.898031",
     "exception": false,
     "start_time": "2022-08-23T08:49:05.973805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import joblib\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "stopwords = list(get_stop_words('en'))\n",
    "\n",
    "def max_repeated_word_count(text):\n",
    "    words = [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "    word_counts = Counter(words)\n",
    "    try:\n",
    "        return word_counts.most_common(1)[0][1]\n",
    "    \n",
    "    except IndexError:\n",
    "        return 0\n",
    "        \n",
    "    return max_count\n",
    "\n",
    "def get_pos_tags(x):\n",
    "    tokens = nltk.tokenize.word_tokenize(x)\n",
    "    tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    return Counter(tag for word, tag in tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cd44f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:10.912114Z",
     "iopub.status.busy": "2022-08-23T08:49:10.911266Z",
     "iopub.status.idle": "2022-08-23T08:49:11.083083Z",
     "shell.execute_reply": "2022-08-23T08:49:11.082118Z"
    },
    "papermill": {
     "duration": 0.18112,
     "end_time": "2022-08-23T08:49:11.085481",
     "exception": false,
     "start_time": "2022-08-23T08:49:10.904361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# char length\n",
    "test_origin['char_length'] = test_origin['discourse_text'].apply(lambda x: len(x))\n",
    "# word count\n",
    "test_origin['word_count'] = test_origin['discourse_text'].apply(lambda x: len(x.split()))\n",
    "# avg word length\n",
    "test_origin['avg_word_length'] = test_origin['char_length'] / test_origin['word_count']\n",
    "# contains 'source'\n",
    "test_origin['contains_source'] = test_origin['discourse_text'].apply(lambda x: 'source' in x.lower().split())\n",
    "# contains 'i'\n",
    "test_origin['contains_I'] = test_origin['discourse_text'].apply(lambda x: 'i' in x.lower().split())\n",
    "# repeated word count\n",
    "test_origin['max_repeated_word_count'] = test_origin['discourse_text'].apply(max_repeated_word_count)\n",
    "\n",
    "# POS\n",
    "test_origin[\"pos_tags\"] = test_origin['discourse_text'].apply(lambda x: get_pos_tags(x))\n",
    "test_origin = pd.concat([test_origin, test_origin[\"pos_tags\"].apply(pd.Series).fillna(0)], axis=1)\n",
    "test_origin.drop(\"pos_tags\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8baaa02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:11.100523Z",
     "iopub.status.busy": "2022-08-23T08:49:11.099477Z",
     "iopub.status.idle": "2022-08-23T08:49:15.325697Z",
     "shell.execute_reply": "2022-08-23T08:49:15.324628Z"
    },
    "papermill": {
     "duration": 4.23659,
     "end_time": "2022-08-23T08:49:15.328462",
     "exception": false,
     "start_time": "2022-08-23T08:49:11.091872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 08:49:12.055908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:12.057024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:12.057777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:12.058888: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 08:49:12.059247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:12.060102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:12.060741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:13.924567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:13.925433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:13.926144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 08:49:13.926876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14867 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-08-23 08:49:14.793775: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encode_cols = ['discourse_type', 'contains_source', 'contains_I']\n",
    "\n",
    "\n",
    "# standard scaler\n",
    "ss_X = joblib.load(f'../input/tf-model-0/scaler.pkl')\n",
    "# tf model\n",
    "model_tf = tf.keras.models.load_model(f'../input/tf-model-0/model')\n",
    "\n",
    "y_preds_0 = []\n",
    "for fold in range(4):\n",
    "\n",
    "    data = test_origin.copy()\n",
    "\n",
    "    mpdel_preds = [model_preds_1[fold], model_preds_2[fold], model_preds_3[fold], model_preds_4[fold], model_preds_5[fold]]\n",
    "    for i, preds in enumerate(mpdel_preds):\n",
    "        col_1 = 'preds' + str(i + 1) + '_' + str(1)\n",
    "        col_2 = 'preds' + str(i + 1) + '_' + str(2)\n",
    "        col_3 = 'preds' + str(i + 1) + '_' + str(3)\n",
    "        data[col_1] = preds[:, 0]\n",
    "        data[col_2] = preds[:, 1]\n",
    "        data[col_3] = preds[:, 2]\n",
    "\n",
    "    cols = ['discourse_type', 'char_length', 'word_count', 'avg_word_length', 'contains_source', 'contains_I', 'max_repeated_word_count', \n",
    "            'preds2_1', 'preds2_2', 'preds2_3', \n",
    "            'preds3_1', 'preds3_2', 'preds3_3', \n",
    "            'preds4_1', 'preds4_2', 'preds4_3', \n",
    "            'preds5_1', 'preds5_2', 'preds5_3',\n",
    "            'ADJ', 'ADV', 'PRON', 'NUM', 'CONJ']\n",
    "    X = data[cols]\n",
    "\n",
    "    for i, col in enumerate(encode_cols):\n",
    "        labelencoder = joblib.load(f'../input/tf-model-0/encoder_{i}.pkl')\n",
    "        X[col] = labelencoder.transform(X[col])\n",
    "\n",
    "    X = ss_X.transform(X)\n",
    "\n",
    "    # predict\n",
    "    y_preds_0.append(model_tf.predict(X))\n",
    "    \n",
    "del model_tf    \n",
    "gc.collect()\n",
    "\n",
    "y_preds_0 = np.mean(y_preds_0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442e0679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:15.344172Z",
     "iopub.status.busy": "2022-08-23T08:49:15.342492Z",
     "iopub.status.idle": "2022-08-23T08:49:16.042198Z",
     "shell.execute_reply": "2022-08-23T08:49:16.041003Z"
    },
    "papermill": {
     "duration": 0.710093,
     "end_time": "2022-08-23T08:49:16.045316",
     "exception": false,
     "start_time": "2022-08-23T08:49:15.335223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "ss_X = joblib.load(f'../input/tf-model-1/scaler.pkl')\n",
    "# tf model\n",
    "model_tf = tf.keras.models.load_model(f'../input/tf-model-1/model')\n",
    "\n",
    "y_preds_1 = []\n",
    "for fold in range(4):\n",
    "\n",
    "    data = test_origin.copy()\n",
    "\n",
    "    mpdel_preds = [model_preds_1[fold], model_preds_2[fold], model_preds_3[fold], model_preds_4[fold], model_preds_5[fold]]\n",
    "    for i, preds in enumerate(mpdel_preds):\n",
    "        col_1 = 'preds' + str(i + 1) + '_' + str(1)\n",
    "        col_2 = 'preds' + str(i + 1) + '_' + str(2)\n",
    "        col_3 = 'preds' + str(i + 1) + '_' + str(3)\n",
    "        data[col_1] = preds[:, 0]\n",
    "        data[col_2] = preds[:, 1]\n",
    "        data[col_3] = preds[:, 2]\n",
    "\n",
    "    cols = ['discourse_type', 'char_length', 'word_count', 'avg_word_length', 'contains_source', 'contains_I', 'max_repeated_word_count', \n",
    "            'preds1_1', 'preds1_2', 'preds1_3', \n",
    "            'preds3_1', 'preds3_2', 'preds3_3', \n",
    "            'preds4_1', 'preds4_2', 'preds4_3', \n",
    "            'preds5_1', 'preds5_2', 'preds5_3',\n",
    "            'ADJ', 'ADV', 'PRON', 'NUM', 'CONJ']\n",
    "    X = data[cols]\n",
    "\n",
    "    for i, col in enumerate(encode_cols):\n",
    "        labelencoder = joblib.load(f'../input/tf-model-1/encoder_{i}.pkl')\n",
    "        X[col] = labelencoder.transform(X[col])\n",
    "\n",
    "    X = ss_X.transform(X)\n",
    "\n",
    "    # predict\n",
    "    y_preds_1.append(model_tf.predict(X))\n",
    "    \n",
    "del model_tf    \n",
    "gc.collect()\n",
    "\n",
    "y_preds_1 = np.mean(y_preds_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33841b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:16.065534Z",
     "iopub.status.busy": "2022-08-23T08:49:16.065207Z",
     "iopub.status.idle": "2022-08-23T08:49:16.771374Z",
     "shell.execute_reply": "2022-08-23T08:49:16.770349Z"
    },
    "papermill": {
     "duration": 0.716263,
     "end_time": "2022-08-23T08:49:16.774169",
     "exception": false,
     "start_time": "2022-08-23T08:49:16.057906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "ss_X = joblib.load(f'../input/tf-model-2/scaler.pkl')\n",
    "# tf model\n",
    "model_tf = tf.keras.models.load_model(f'../input/tf-model-2/model')\n",
    "\n",
    "y_preds_2 = []\n",
    "for fold in range(4):\n",
    "\n",
    "    data = test_origin.copy()\n",
    "\n",
    "    mpdel_preds = [model_preds_1[fold], model_preds_2[fold], model_preds_3[fold], model_preds_4[fold], model_preds_5[fold]]\n",
    "    for i, preds in enumerate(mpdel_preds):\n",
    "        col_1 = 'preds' + str(i + 1) + '_' + str(1)\n",
    "        col_2 = 'preds' + str(i + 1) + '_' + str(2)\n",
    "        col_3 = 'preds' + str(i + 1) + '_' + str(3)\n",
    "        data[col_1] = preds[:, 0]\n",
    "        data[col_2] = preds[:, 1]\n",
    "        data[col_3] = preds[:, 2]\n",
    "\n",
    "    cols = ['discourse_type', 'char_length', 'word_count', 'avg_word_length', 'contains_source', 'contains_I', 'max_repeated_word_count', \n",
    "            'preds1_1', 'preds1_2', 'preds1_3', \n",
    "            'preds2_1', 'preds2_2', 'preds2_3', \n",
    "            'preds3_1', 'preds3_2', 'preds3_3', \n",
    "            'preds4_1', 'preds4_2', 'preds4_3',\n",
    "            'ADJ', 'ADV', 'PRON', 'NUM', 'CONJ']\n",
    "    X = data[cols]\n",
    "\n",
    "    for i, col in enumerate(encode_cols):\n",
    "        labelencoder = joblib.load(f'../input/tf-model-2/encoder_{i}.pkl')\n",
    "        X[col] = labelencoder.transform(X[col])\n",
    "\n",
    "    X = ss_X.transform(X)\n",
    "\n",
    "    # predict\n",
    "    y_preds_2.append(model_tf.predict(X))\n",
    "    \n",
    "del model_tf    \n",
    "gc.collect()\n",
    "\n",
    "y_preds_2 = np.mean(y_preds_2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f3640",
   "metadata": {
    "papermill": {
     "duration": 0.00675,
     "end_time": "2022-08-23T08:49:16.787503",
     "exception": false,
     "start_time": "2022-08-23T08:49:16.780753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ff1e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T08:49:16.801147Z",
     "iopub.status.busy": "2022-08-23T08:49:16.800841Z",
     "iopub.status.idle": "2022-08-23T08:49:16.831994Z",
     "shell.execute_reply": "2022-08-23T08:49:16.830626Z"
    },
    "papermill": {
     "duration": 0.041209,
     "end_time": "2022-08-23T08:49:16.834867",
     "exception": false,
     "start_time": "2022-08-23T08:49:16.793658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>Ineffective</th>\n",
       "      <th>Adequate</th>\n",
       "      <th>Effective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a261b6e14276</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>0.497955</td>\n",
       "      <td>0.486870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a88900e7dc1</td>\n",
       "      <td>0.028157</td>\n",
       "      <td>0.781238</td>\n",
       "      <td>0.190605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9790d835736b</td>\n",
       "      <td>0.027453</td>\n",
       "      <td>0.420099</td>\n",
       "      <td>0.552448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75ce6d68b67b</td>\n",
       "      <td>0.080789</td>\n",
       "      <td>0.435147</td>\n",
       "      <td>0.484064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93578d946723</td>\n",
       "      <td>0.074161</td>\n",
       "      <td>0.510287</td>\n",
       "      <td>0.415552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2e214524dbe3</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.478236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84812fc2ab9f</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.450413</td>\n",
       "      <td>0.527949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c668ff840720</td>\n",
       "      <td>0.060464</td>\n",
       "      <td>0.614413</td>\n",
       "      <td>0.325123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>739a6d00f44a</td>\n",
       "      <td>0.052531</td>\n",
       "      <td>0.537010</td>\n",
       "      <td>0.410459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bcfae2c9a244</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>0.608356</td>\n",
       "      <td>0.368584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id  Ineffective  Adequate  Effective\n",
       "0  a261b6e14276     0.015175  0.497955   0.486870\n",
       "1  5a88900e7dc1     0.028157  0.781238   0.190605\n",
       "2  9790d835736b     0.027453  0.420099   0.552448\n",
       "3  75ce6d68b67b     0.080789  0.435147   0.484064\n",
       "4  93578d946723     0.074161  0.510287   0.415552\n",
       "5  2e214524dbe3     0.021663  0.500101   0.478236\n",
       "6  84812fc2ab9f     0.021637  0.450413   0.527949\n",
       "7  c668ff840720     0.060464  0.614413   0.325123\n",
       "8  739a6d00f44a     0.052531  0.537010   0.410459\n",
       "9  bcfae2c9a244     0.023060  0.608356   0.368584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_preds_1_mean = np.mean(model_preds_1, axis=0)\n",
    "preds_Ineffective_0 = y_preds_0[:, 0] * 0.75 + model_preds_1_mean[:, 2] * 0.25\n",
    "preds_Adequate_0 = y_preds_0[:, 1] * 0.75 + model_preds_1_mean[:, 0] * 0.25\n",
    "preds_Effective_0 = y_preds_0[:, 2] * 0.75 + model_preds_1_mean[:, 1] * 0.25\n",
    "\n",
    "model_preds_2_mean = np.mean(model_preds_2, axis=0)\n",
    "preds_Ineffective_1 = y_preds_1[:, 0] * 0.75 + model_preds_2_mean[:, 2] * 0.25\n",
    "preds_Adequate_1 = y_preds_1[:, 1] * 0.75 + model_preds_2_mean[:, 0] * 0.25\n",
    "preds_Effective_1 = y_preds_1[:, 2] * 0.75 + model_preds_2_mean[:, 1] * 0.25\n",
    "\n",
    "model_preds_5_mean = np.mean(model_preds_5, axis=0)\n",
    "preds_Ineffective_2 = y_preds_2[:, 0] * 0.75 + model_preds_5_mean[:, 0] * 0.25\n",
    "preds_Adequate_2 = y_preds_2[:, 1] * 0.75 + model_preds_5_mean[:, 1] * 0.25\n",
    "preds_Effective_2 = y_preds_2[:, 2] * 0.75 + model_preds_5_mean[:, 2] * 0.25\n",
    "\n",
    "preds_Ineffective = preds_Ineffective_0 * 0.33 + preds_Ineffective_1 * 0.34 + preds_Ineffective_2 * 0.33\n",
    "preds_Adequate = preds_Adequate_0 * 0.33 + preds_Adequate_1 * 0.34 + preds_Adequate_2 * 0.33\n",
    "preds_Effective = preds_Effective_0 * 0.33 + preds_Effective_1 * 0.34 + preds_Effective_2 * 0.33\n",
    "\n",
    "sample = pd.read_csv(INPUT_DIR + 'sample_submission.csv')\n",
    "\n",
    "sample['Ineffective'] = preds_Ineffective\n",
    "sample['Adequate'] = preds_Adequate\n",
    "sample['Effective'] = preds_Effective\n",
    "\n",
    "sample.to_csv('submission.csv', index=False)\n",
    "\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6538e5",
   "metadata": {
    "papermill": {
     "duration": 0.006067,
     "end_time": "2022-08-23T08:49:16.850362",
     "exception": false,
     "start_time": "2022-08-23T08:49:16.844295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 793.626652,
   "end_time": "2022-08-23T08:49:20.152485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-23T08:36:06.525833",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
